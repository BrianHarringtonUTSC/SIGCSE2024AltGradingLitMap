@article{alvarez2023modularization,
  title={Modularization for mastery learning in CS1: a 4-year action research study},
  author={Alvarez, Claudio and Samary, Maira Marques and Wise, Alyssa Friend},
  journal={Journal of Computing in Higher Education},
  pages={1--44},
  year={2023},
  publisher={Springer}
}


@article{cheng2021effects,
author = {Li-Chen Cheng, Wei Li and Judy C. R. Tseng},
title = {Effects of an automated programming assessment system on the learning performances of experienced and novice learners},
journal = {Interactive Learning Environments},
volume = {0},
number = {0},
pages = {1-17},
year = {2021},
publisher = {Routledge},
doi = {10.1080/10494820.2021.2006237},

}

@inproceedings{jazayeri2015combining,
author = {Jazayeri, Mehdi},
title = {Combining Mastery Learning with Project-Based Learning in a First Programming Course: An Experience Report},
year = {2015},
publisher = {IEEE Press},
abstract = {One of the challenges in teaching a first programming course is that in the same course, the students must learn basic programming techniques and high level abstraction abilities, and the application of those techniques and concepts in problem solving and (engineering) design. To confront this challenge, in previous years, we have included a project-based learning phase at the end of our course to encourage the acquisition of high level design and creativity. To address some of the shortcomings of our previous editions, we have recently included a mastery phase to the course. While project-based learning is suitable for teaching high-level skills that require design and creativity and prepare the students for the study of software engineering, mastery-based learning is suitable for concrete skills such as basic programming tasks. Our particular innovation is to allow students into the project phase only if they have demonstrated a minimum predefined competency level in programming. The combination of the two approaches seems to address most of the requirements of a first programming course. We present our motivation for combining the two pedagogical techniques and our experience with the course.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 2},
pages = {315–318},
numpages = {4},
keywords = {introductory programming, project-based learning, software engineering education, CS1/CS2, first programming course, mastery learning},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{capovilla2015handling,
author = {Capovilla, Dino and Berges, Marc and M\"{u}hling, Andreas and Hubwieser, Peter},
title = {Handling Heterogeneity in Programming Courses for Freshmen},
year = {2015},
isbn = {9781479999675},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/LaTiCE.2015.18},
doi = {10.1109/LaTiCE.2015.18},
abstract = {One of the biggest challenges of the computer science department at our university is handling the enormous heterogeneity of freshmen concerning both their previous programming abilities and their learning behaviors due to their biographical and social background. In this paper, we present the design and evaluation of a preliminary programming course based on the teaching method of Mastery Learning that is particularly suited for groups of students characterized by considerable diversity. Trained peer tutors closely guide the participants through a step-by-step programming exercise. We tested the method in several courses that ran for two and a half days four weeks before the start of lectures. We collected data from two different surveys(N = 200 and N = 300, respectively). First, we quantified the considerable differences concerning the prior experience in programming of the participants. Second, we succeeded to show that the outcome of our method is independent from different sensory preferences and different computer-usage behaviors of the students. Third, the results of the survey demonstrate that our method is suited to increasing the self-perception of programming ability. This helps freshmen to overcom initial self-doubts when beginning their CS studies.},
booktitle = {Proceedings of the 2015 International Conference on Learning and Teaching in Computing and Engineering},
pages = {197–203},
numpages = {7},
keywords = {Learning styles, Mastery Learning, CS education},
series = {LATICE '15}
}

@INPROCEEDINGS{mckell2020exploring,
  author={McKell, K. Clay and Danowitz, Andrew},
  booktitle={2020 IEEE Frontiers in Education Conference (FIE)}, 
  title={Exploring the effect of standards-based grading on student learning}, 
  year={2020},
  volume={},
  number={},
  pages={1-7},
  doi={10.1109/FIE44824.2020.9273889}}


@inproceedings{baniassad2019teaching,
author = {Baniassad, Elisa and Campbell, Alice and Allidina, Tiara and Ord, Asrai},
title = {Teaching Software Construction at Scale with Mastery Learning: A Case Study},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEET.2019.00027},
doi = {10.1109/ICSE-SEET.2019.00027},
abstract = {Mastery Learning involves delineating learning units and assessing each unit individually and repeatedly until a student obtains success. Mastery Learning has been shown to help students better identify and grasp fundamental concepts. We applied Mastery Learning in a second-year software construction course with roughly 450 students. We delineated 23 topics, and administered either a written or verbal quiz to assess each topic. We built a quiz auto-grading, analysis, visualization, and feedback system to help cope with the scale of the class. By the end of the semester we had administered over 12K quizzes. We found evidence that students grasped both fundamental concepts and advanced concepts better than in prior semesters. Because we made two changes at once (introducing videos and Mastery Learning) it is difficult to isolate whether the Mastery Learning Approach was solely responsible, but assessment results suggest that the repeatable micro-quizzes were instrumental in these gains. Auto-grading and extensive data collection allowed a depth of analysis that afforded us invaluable and lasting pedagogical insights.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: Software Engineering Education and Training},
pages = {182–191},
numpages = {10},
location = {Montreal, Quebec, Canada},
series = {ICSE-SEET '19}
}

@article{gestwicki2021godot,
author = {Gestwicki, Paul},
title = {Godot Engine and Checklist-Based Specifications: Revising a Game Programming Class for Asynchronous Online Teaching},
year = {2021},
issue_date = {October 2021},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {37},
number = {4},
issn = {1937-4771},
abstract = {This experience report describes the revisions to an undergraduate elective game programming course that were made in response to the COVID-19 pandemic. The course transitioned from a lab-based, in-person class to an online, asynchronous one. This required a change in teaching technology from Unreal Engine 4 to Godot Engine. The course expanded its use of checklist-based specifications grading in order to facilitate student autonomy with minimal reduction in creativity and motivation. The course revisions required significant time investment, but the results were positive.},
journal = {J. Comput. Sci. Coll.},
month = {oct},
pages = {30–40},
numpages = {11}
}

@article{sanft2021modified,
author = {Sanft, Kevin R. and Drawert, Brian and Whitley, Adam},
title = {Modified Specifications Grading in Computer Science: Preliminary Assessment and Experience across Five Undergraduate Courses},
year = {2021},
issue_date = {January 2021},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {36},
number = {5},
issn = {1937-4771},
abstract = {Specifications grading has been proposed as a method to improve student outcomes and reduce faculty grading time. There are many ways to implement specifications grading that differ in their details, but the overarching philosophy is that student work receives credit only when it fully satisfies a given set of requirements. Therefore, grading requires only a satisfactory/unsatisfactory designation, reducing grading time. By allowing students to resubmit assignments, it encourages mastery of the material. We present a version of specifications grading that was implemented across five undergraduate computer science courses over four semesters, comprising twelve total course offerings. We find evidence for improved student outcomes, especially among middle to low performing students. Grading time was reduced for most programming assignments. Assignments with many small independent parts, such as short answer textbook problems, generally did not lead to grading time savings. Student enjoyment of specifications grading when compared to traditional grading was polarized with some students strongly disliking it. Overall, the modified specifications grading scheme presented here offers a number of benefits to students and faculty that make it an appealing option for undergraduate computer science courses.},
journal = {J. Comput. Sci. Coll.},
month = {jan},
pages = {34–46},
numpages = {13}
}

@article{mirsky2018effectiveness,
author = {Mirsky, Grace M.},
title = {Effectiveness of Specifications Grading in Teaching Technical Writing to Computer Science Students},
year = {2018},
issue_date = {October 2018},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {34},
number = {1},
issn = {1937-4771},
abstract = {Successfully motivating computer science students to improve their technical writing skills is a challenging task. This study investigates the effectiveness of specifications grading, a relatively new grading technique, in improving the writing skills of computer science students in a writing intensive Computer Architecture course. It is important to note that while the objective of this work was to improve technical writing, the grading technique is widely applicable to a variety of courses. Rather than creating a rubric involving partial credit, writing assignments were graded as "all or nothing" for two categories, writing quality and content, under the guidance of clear standards. For writing quality the requirement for credit was no more than five errors; for content it was no more than three poorly explained and/or omitted topics. Students meeting the standard received all the points for that category; students not meeting the standard received zero points for that category. Overall, students in two sections of this course responded very well to this grading strategy both by significantly improving their writing quality over the course of the semester as well as in their strongly positive opinions of their learning in the end of course assessments. Another significant benefit of specifications grading is that it affords faculty a considerable reduction in grading workload, while maintaining desirable educational outcomes. While more research is necessary to thoroughly evaluate its efficacy, specifications grading is promising as a motivational grading methodology.},
journal = {J. Comput. Sci. Coll.},
month = {oct},
pages = {104–110},
numpages = {7}
}

@article{engle2013expert,
author = {Engle, Sophie and Rollins, Sami},
title = {Expert Code Review and Mastery Learning in a Software Development Course},
year = {2013},
issue_date = {April 2013},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {28},
number = {4},
issn = {1937-4771},
abstract = {For students to be successful in upper-division courses and as junior developers, they must master concepts such as code design and concurrency. However, traditional grading and partial credit often allows students to pass courses without demonstrating appropriate mastery. This paper reports on our experience applying mastery learning and expert code review to our software development course. We compare two consecutive semesters of this course---one using a traditional approach and the other using mastery learning and expert code review. We discuss our experience setting student expectations, the differences in grades and code quality between the semesters, and provide recommendations on how to improve and adapt this approach for other courses.},
journal = {J. Comput. Sci. Coll.},
month = {apr},
pages = {139–147},
numpages = {9}
}

@article{lejune2010contract,
author = {LeJeune, Noel},
title = {Contract Grading with Mastery Learning in CS 1},
year = {2010},
issue_date = {December 2010},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {26},
number = {2},
issn = {1937-4771},
abstract = {Mastery learning as an educational technique can be successfully used in a CS 1 course. Instructors explicitly predetermine the skills and knowledge required for varying levels of mastery. Incorporating contract grading with the levels of mastery gives students control over the grade they earn. Students have a high level of satisfaction when given clear and commonly understood expectations for performance when also coupled with the ability to choose their level of achievement. In an experiment with five sections of CS 1 over two consecutive semesters, students indicated a strong preference for mastery learning with contract grading over traditional percentage grading. Students' self-reported beliefs were that they did slightly more work, learned more, learned it better, and learned it more easily because of mastery learning and contract grading. They strongly felt that they were in control of the grade they received and were very confident that they knew what grade they would earn. Students strongly believed that the mastery learning aspect resulted in improved learning and higher grades. The technique used for mastery learning with contract grading is described along with significant benefits and problems encountered.},
journal = {J. Comput. Sci. Coll.},
month = {dec},
pages = {149–156},
numpages = {8}
}

@article{lawrence2023evaluation,
author = {Lawrence, Ramon and Foss, Sarah and Urazova, Tatiana},
title = {Evaluation of Submission Limits and Regression Penalties to Improve Student Behavior with Automatic Assessment Systems},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
url = {https://doi.org/10.1145/3591210},
doi = {10.1145/3591210},
abstract = {Objectives. Automatic assessment systems are widely used to provide rapid feedback for students and reduce grading time. Despite the benefits of increased efficiency and improved pedagogical outcomes, an ongoing challenge is mitigating poor student behaviors when interacting with automatic assessment systems including numerous submissions, trial-and-error, and relying on marking feedback for problem solving. These behaviors negatively affect student learning as well as have significant impact on system resources. This research quantitatively examines how utilizing submission policies such as limiting the number of submissions and applying regression penalties can reduce negative student behaviors. The hypothesis is that both submission policies will have a significant impact on student behavior and reduce both the number of submissions and regressions in student performance. The research questions evaluate the impact on student behavior, determine which submission policy is the most effective, and what submission policy is preferred by students.Participants. The study involved two course sections in two different semesters consisting of a total of 224 students at the University of British Columbia, a research-intensive university. The students were evaluated using an automated assessment system in a large third year database course.Study Methods. The two course sections used an automated assessment system for constructing database design diagrams for assignments and exams. The first section had no limits on the number of submissions for both assignments and exams. The second section had limits for the exams but no limits on assignments. On the midterm, participants were randomly assigned to have either a restriction on the total number of submissions or unlimited submissions but with regression penalties if a graded answer was lower than a previous submission. On the final exam, students were given the option of selecting their submission policy. Student academic performance and submission profiles were compared between the course sections and the different submission policies.Findings. Unrestricted use of automatic grading systems results in high occurrence of undesirable student behavior including trial-and-error guessing and reduced time between submissions without sufficient independent thought. Both submission policies of limiting maximum submissions and utilizing regression penalties significantly reduce these behaviors by up to 85\%. Overall, students prefer maximum submission limits, and demonstrate improved behavior and educational outcomes.Conclusions. Automated assessment systems when used for larger problems related to design and programming have benefits when deployed with submission restrictions (maximum attempts or regression penalty) for both improved student learning behaviors and to reduce the computational costs for the system. This is especially important for summative assessment but reasonable limits for formative assessments are also valuable.},
journal = {ACM Trans. Comput. Educ.},
month = {jun},
articleno = {31},
numpages = {24},
keywords = {assessment, database, Automatic marking, submission limits, auto-grading, UML, regression penalties, programming and design questions}
}

@article{assiter2023integrating,
author = {Assiter, Karina V.},
title = {Integrating Grading for Equity Practices into Project-Based Computer Science Curriculum},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {2153-2184},
url = {https://doi.org/10.1145/3582559},
doi = {10.1145/3582559},
journal = {ACM Inroads},
month = {feb},
pages = {22–29},
numpages = {8}
}

@inproceedings{tuson2023mastery,
author = {Tuson, Ella and Hickey, Timothy},
title = {Mastery Learning with Specs Grading for Programming Courses},
year = {2023},
isbn = {9781450394314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545945.3569853},
doi = {10.1145/3545945.3569853},
abstract = {As professors, we want the students in our classes to succeed in mastering the material that we set out to teach them, but we must balance this desire with the knowledge that we have other responsibilities and a limited number of hours in the day. In this report, we document our implementation of a mastery learning inspired pedagogy using specifications grading in a software engineering course from the spring semester of 2022 in which 142 students were enrolled. Our two main goals with this approach were to reduce the administrative burden of the class with respect to grading and to promote mastery of course material while maintaining academic rigor. We provide evidence that both of these goals were at least partially achieved. In addition to outlining the structure of the course, we identify several areas where there is room for improvement with this approach and provide an overview of an online application we developed to facilitate the course.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1049–1054},
numpages = {6},
keywords = {specification grading, mastery learning},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{lionelle2023flexible,
author = {Lionelle, Albert and Ghosh, Sudipto and Moraes, Marcia and Winick, Tran and Nielsen, Lindsey},
title = {A Flexible Formative/Summative Grading System for Large Courses},
year = {2023},
isbn = {9781450394314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545945.3569810},
doi = {10.1145/3545945.3569810},
abstract = {Students in entry level CS courses come from diverse backgrounds and are learning study and time management skills. Our belief for their success is that they must master a growth mindset and that the final grade should represent their final mastery of topics in the course. Traditional grading systems tend to be too restrictive and hinder a growth mindset. They require strict deadlines that fail to easily account for student accommodations and learning differences. Furthermore, they run into averaging and scaling issues with 59\% of a score counting as failing, making it difficult for students to redeem grades even if they later demonstrate mastery of topics.We designed a formative/summative grading system in our CS0 and CS1 classes for both on-campus and online students to support a structured growth mindset. Students can redo formative assignments and are provided flexible deadlines. They demonstrate their mastery in summative assignments. While being inspired by other grading systems, our system works seamlessly with auto-grading tools used in large, structured courses. Despite the flexibility, the courses provided a level of rigor before allowing students to continue onto the next course.Overall, 65\% of students resubmitted assignments increasing their scores, participated in ungraded assignments, and used formative assignments for additional practice without a distinction between race or gender. These students went to the traditional follow-on CS2 course and 94\% passed compared with 71\% who took CS1 with a traditional grading system.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
pages = {624–630},
numpages = {7},
keywords = {grading, computing education, cs 0, cs1, growth mindset},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{weber2023using,
author = {Weber, Robbie},
title = {Using Alternative Grading in a Non-Major Algorithms Course},
year = {2023},
isbn = {9781450394314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545945.3569765},
doi = {10.1145/3545945.3569765},
abstract = {We implemented a standards-based grading scheme in an upperdivision course on algorithm design taken by non-CS-majors. The alternate grading system allows students to submit multiple attempts at the same algorithm design problem, while managing grading load by replacing standard point-based scales with a 4- possibility-scale for all problems. The simplified grading system created flexibility in the course structure that allowed us to give students more problems each week than we expected them to complete, covering different aspects of the given topics (e.g., both theoretical and practical approaches to algorithm design). The additional problems allowed for students with different goals and backgrounds to choose different problems and tailor it to their needs. The availability of resubmissions created incentives for students to master difficult topics throughout the term without a final exam. We argue that the simplified grading system is particularly well-suited to courses in algorithm design and courses for students with varying backgrounds and goals.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
pages = {638–644},
numpages = {7},
keywords = {assessment, experience report, non-majors, mastery grading, standards-based grading, algorithms},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{spurlock2023improving,
author = {Spurlock, Scott},
title = {Improving Student Motivation by Ungrading},
year = {2023},
isbn = {9781450394314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545945.3569747},
doi = {10.1145/3545945.3569747},
abstract = {Recent interest in alternative grading strategies has been increasing in the Computer Science Education community. The umbrella term ungrading has been used to refer to a variety of practices that de-emphasize numeric grades. In this paper we present the results of implementing an ungrading scheme that eliminates numeric grades, allows resubmission of assignments, and encourages student input into their final assigned letter grade. We administered surveys measuring student attitudes and motivation at the start and end of three different upper level Computer Science elective courses using the new grading scheme and found a significant increase in students' feelings of intrinsic goal orientation (valuing coursework for its own sake), self-efficacy (feeling able to be successful), and control of learning (taking responsibility for their own learning). We observed that, given the opportunity, most students chose to redo only a small number of assignments, and most students requested final grades within a half-letter of the instructor's estimate. Overall, compared with prior iterations of the courses that were graded traditionally, the final grade point average did not significantly increase, while students' reported level of effort did significantly increase. Comments on post-course surveys indicate that students liked the new grading scheme, and they reported improved learning and reduced anxiety.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
pages = {631–637},
numpages = {7},
keywords = {pedagogy, ungrading, assessment, computer science education},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{tuson2022mastery,
author = {Tuson, Ella and Hickey, Tim},
title = {Mastery Learning and Specs Grading in Discrete Math},
year = {2022},
isbn = {9781450392013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3502718.3524766},
doi = {10.1145/3502718.3524766},
abstract = {This paper presents a case study detailing our experience applying a combination of Mastery Learning and Specs Grading to a section of a Discrete Mathematics course with 128 students. Our principle reason to use this pedagogy was to improve the learning outcomes of our students, so all students would have a good chance of succeeding, regardless of their previous experience. The course was focused on 10 main skill areas. Each week a new skill area was introduced and a quiz for that skill area was provided that week and each week thereafter. The quizzes were graded pass/fail, either they demonstrated complete mastery or they did not. Students who demonstrated mastery were no longer required to take the quizzes on that skill area in later weeks. The amount of extra work required for this approach with regards to grading was roughly twice as much as would have been required with a traditional midterm/final exam structure and three times as much for creating quiz questions. Despite this increase in the number of items that required grading, the overall time spent grading was greatly reduced due to the use of pass/fail grading. The Mastery Learning approach provided a strong incentive for students to attempt to master all of the core skill areas. By the end of the semester, 75\% of the students had mastered at least nine of the ten skill areas. In this paper we discuss this approach and its implications for CS courses more generally.},
booktitle = {Proceedings of the 27th ACM Conference on on Innovation and Technology in Computer Science Education Vol. 1},
pages = {19–25},
numpages = {7},
keywords = {high frequency low stakes testing, binary grading, specifications grading, mastery learning, pass/fail grading},
location = {Dublin, Ireland},
series = {ITiCSE '22}
}

@inproceedings{ott2021mastery,
author = {Ott, Claudia and McCane, Brendan and Meek, Nick},
title = {Mastery Learning in CS1 - An Invitation to Procrastinate?: Reflecting on Six Years of Mastery Learning},
year = {2021},
isbn = {9781450382144},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430665.3456321},
doi = {10.1145/3430665.3456321},
abstract = {Over six years we developed our first-year programming course, delivered through scheduled lectures and assessed by practical tests and a final examination, into a mastery learning-oriented course. In this study we provide an in-depth view of how successive adjustments to the course resulted in changes in student behaviour impacting task completion, final grades and activity patterns. Data from this period involving over 1300 students is presented and augmented with student feedback. Our results show that the successive move to a fully-fledged mastery model resulted in an overall decreasing task completion with longer periods of inactivity. Interventions to increase student engagement with the course were only partly successful. In this paper we present a long-term study to highlight opportunities and challenges when shifting to a mastery learning model.},
booktitle = {Proceedings of the 26th ACM Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {18–24},
numpages = {7},
keywords = {student engagement, introductory programming, mastery learning, procrastination},
location = {Virtual Event, Germany},
series = {ITiCSE '21}
}

@inproceedings{zamprogno2020nudging,
author = {Zamprogno, Lucas and Holmes, Reid and Baniassad, Elisa},
title = {Nudging Student Learning Strategies Using Formative Feedback in Automatically Graded Assessments},
year = {2020},
isbn = {9781450381802},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426431.3428654},
doi = {10.1145/3426431.3428654},
abstract = {Automated assessment tools are widely used as a means for providing formative feedback to undergraduate students in computer science courses while helping those courses simultaneously scale to meet student demand. While formative feedback is a laudable goal, we have observed many students trying to debug their solutions into existence using only the feedback given, while losing context of the learning goals intended by the course staff. In this paper, we detail two case studies from second and third-year undergraduate software engineering courses indicating that using only nudges about where students should focus their efforts can improve how they act on generated feedback. By carefully reasoning about errors uncovered by our automated assessment approaches, we have been able to create feedback for students that helps them to revisit the learning outcomes for the assignment or course. This approach has been applied to both multiple-choice feedback in an online quiz taking system and automated assessment of student programming tasks. We have found that student performance has not suffered and that students reflect positively about how they investigate automated assessment failures.},
booktitle = {Proceedings of the 2020 ACM SIGPLAN Symposium on SPLASH-E},
pages = {1–11},
numpages = {11},
keywords = {assessment, software engineering, autograder},
location = {Virtual, USA},
series = {SPLASH-E 2020}
}

@inproceedings{bridson2021frequent,
author = {Bridson, Kathryn and Fleming, Scott D.},
title = {Frequent, Timed Coding Tests for Training and Assessment of Full-Stack Web Development Skills: An Experience Report},
year = {2021},
isbn = {9781450380621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408877.3432549},
doi = {10.1145/3408877.3432549},
abstract = {This experience report describes the use of frequent, timed coding tests in a project-intensive software engineering course in which students first learn full-stack web development using Ruby on Rails and then apply their skills in a team project. The goal of the skills tests was twofold: (1) to help motivate students to engage in distributed practice and, thus, gain adequate coding skills to be an effective team member during the team project and (2) to accurately assess whether students had acquired the requisite skills and, thereby, catch deficiencies early, while there was still time to address them. Regarding the first goal, although several students indicated that the tests motivated them to engage in substantial practice coding, it was ultimately inconclusive as to the extent of the tests' impact on students' distributed practice behavior and on their preparation for the project. Regarding the second goal, the skills testing approach was indeed considerably more effective than graded homework assignments for assessing coding skill and detecting struggling students early. Lessons learned from our experiences included that students had significant concerns about the strict time limit on the tests, that the tests caused a spike in mid-semester withdrawals from the course that disproportionately impacted students from underrepresented groups, and that detecting struggling students was one thing, but effectively helping them catch up was a whole other challenge.},
booktitle = {Proceedings of the 52nd ACM Technical Symposium on Computer Science Education},
pages = {24–30},
numpages = {7},
keywords = {software engineering education, assessment, full-stack web development, skills testing, mastery learning},
location = {Virtual Event, USA},
series = {SIGCSE '21}
}

@inproceedings{herman2020comparison,
author = {Herman, Geoffrey L. and Cai, Zhouxiang and Bretl, Timothy and Zilles, Craig and West, Matthew},
title = {Comparison of Grade Replacement and Weighted Averages for Second-Chance Exams},
year = {2020},
isbn = {9781450370929},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372782.3406260},
doi = {10.1145/3372782.3406260},
abstract = {We explore how course policies affect students' studying and learning when a second-chance exam is offered. High-stakes, one-off exams remain a de facto standard for assessing student knowledge in STEM, despite compelling evidence that other assessment paradigms such as mastery learning can improve student learning. Unfortunately, mastery learning can be costly to implement. We explore the use of optional second-chance testing to sustainably reap the benefits of mastery-based learning at scale. Prior work has shown that course policies affect students' studying and learning but have not compared these effects within the same course context. We conducted a quasi-experimental study in a single course to compare the effect of two grading policies for second-chance exams and the effect of increasing the size of the range of dates for students taking asynchronous exams. The first grading policy, called 90-cap, allowed students to optionally take a second-chance exam that would fully replace their score on a first-chance exam except the second-chance exam would be capped at 90\% credit. The second grading policy, called 90-10, combined students' first- and second-chance exam scores as a weighted average (90\% max score + 10\% min score). The 90-10 policy significantly increased the likelihood that marginally competent students would take the second-chance exam. Further, our data suggests that students learned more under the 90-10 policy, providing improved student learning outcomes at no cost to the instructor. Most students took exams on the last day an exam was available, regardless of how many days the exam was available.},
booktitle = {Proceedings of the 2020 ACM Conference on International Computing Education Research},
pages = {56–66},
numpages = {11},
keywords = {mastery, assessment, computer-based exams, second-chance testing, computer education},
location = {Virtual Event, New Zealand},
series = {ICER '20}
}

@inproceedings{berns2020scored,
author = {Berns, Andrew},
title = {Scored out of 10: Experiences with Binary Grading Across the Curriculum},
year = {2020},
isbn = {9781450367936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328778.3366956},
doi = {10.1145/3328778.3366956},
abstract = {Perhaps no other task has frustrated us as instructors as much as grading. While we recognize the importance of providing meaningful feedback and accurate assessment, our traditional grading had quickly become a task of administrative minutiae instead of helpful coaching. Like many faculty, we have searched for ways to reduce our grading load without harming student learning. We have tried a variety of approaches, from modifying our existing assignments all the way to rethinking our entire course design. The past few years we have been refining a binary grading system : a set of grading principles based upon the practice of scoring student assessments simply as either satisfactory or unsatisfactory ($10_2$ possible scores, thus the paper title). We believe this approach has significantly improved our students' attitudes towards our courses and holds promise as a way to significantly reduce the time we spend grading. In this paper, we describe our experiences with our binary grading system in several undergraduate computer science courses. We discuss the general practices of our binary grading, how we have implemented it in our courses, and what our experiences have taught us regarding the future refinement of our grading practices. We hope our experiences will spur discussion and assist other computer science educators in improving their assessment process.},
booktitle = {Proceedings of the 51st ACM Technical Symposium on Computer Science Education},
pages = {1152–1157},
numpages = {6},
keywords = {grading techniques, student assessment, course administration},
location = {Portland, OR, USA},
series = {SIGCSE '20}
}

@inproceedings{carmosino2020adaptive,
author = {Carmosino, Marco and Minnes, Mia},
title = {Adaptive Rubrics},
year = {2020},
isbn = {9781450367936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328778.3366946},
doi = {10.1145/3328778.3366946},
abstract = {Grading is a notoriously difficult and time-consuming part of teaching. For open-ended programming, mathematical, or design problems, assigning consistent scores and giving useful feedback can be very challenging. Large classes compound this difficulty. Adding TAs to the team can help parallelize the process but may impede grading consistency and quality. We present an adaptive rubric creation and application process to enable high-quality responses to student work, at scale. This process uses exploratory data analysis to discover common patterns in student responses to a problem, then tailors a rubric and feedback to address these patterns. Our method is supported by current grading tools, which allow calculation of the simple population-level statistics we need to extract meaningful features from a corpus of student work. In this case study, we describe using adaptive rubrics for a discrete math class for CS majors: the grading team found that this process produced concrete and transparent justifications of student scores and that it facilitated conversations around grading that were grounded in course learning objectives and values.},
booktitle = {Proceedings of the 51st ACM Technical Symposium on Computer Science Education},
pages = {549–555},
numpages = {7},
keywords = {large classes, rubrics, educational data mining, grading},
location = {Portland, OR, USA},
series = {SIGCSE '20}
}

@inproceedings{weikle2019automating,
author = {Weikle, Dee A. B. and Lam, Michael O. and Kirkpatrick, Michael S.},
title = {Automating Systems Course Unit and Integration Testing: Experience Report},
year = {2019},
isbn = {9781450358903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287324.3287502},
doi = {10.1145/3287324.3287502},
abstract = {Introducing software testing has taken on a greater importance in undergraduate computer science curricula in the last several years, with many departments using JUnit or other testing frameworks in the programming sequence and software engineering courses. We have developed an automated framework for unit and integration testing and grading for our intermediate-level systems course projects. Our system--designed to test C programs--combines the Check unit testing framework, custom Bash scripts for integration testing, and the Valgrind Memcheck memory leak detection tool. Although our courses use Linux, the framework is platform-independent and has been tested on a variety of other platforms. We have used this framework for seven semesters with four different instructors as part of the computer science program at a primarily undergraduate university with an emphasis on liberal arts. We distribute both public and private tests so that students get immediate feedback on their progress without knowing the actual contents of every test. We have observed that knowing their code is not completely working motivates more students to figure out what they don't understand before the project deadline. It also gives students examples of different levels of tests to use to debug their code, encourages them to develop a deeper understanding of the project specification, and reduces student anxiety about grades.},
booktitle = {Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
pages = {565–570},
numpages = {6},
keywords = {unit testing, computer science education, computer systems fundamentals},
location = {Minneapolis, MN, USA},
series = {SIGCSE '19}
}

@inproceedings{campbell2019selfpaced,
author = {Campbell, Jennifer and Petersen, Andrew and Smith, Jacqueline},
title = {Self-Paced Mastery Learning CS1},
year = {2019},
isbn = {9781450358903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287324.3287481},
doi = {10.1145/3287324.3287481},
abstract = {This report documents the implementation of a self-paced, mastery learning inspired CS1 course. The course was designed to increase the completion rates observed in flipped and online CS1 formats already offered at our institution. We explore the experience of students in the course and evaluate performance outcomes using grade data from all three CS1 formats, student survey responses, and exit interviews. Our evaluation identifies three main challenges in our implementation. First, the course requires significant resources and administering it is significantly more time consuming for instructors than a regular course. Second, students hesitated to treat mastery quizzes as formative. Finally, the flexibility that the course provided, with little structure and few incentives to help students stay on track, led to considerable procrastination. These factors combined to lead students to delay coursework until the end of the semester -- and beyond. As a result, while our data shows an increase in completion relative to the online format, we saw no change in completion relative to the flipped CS1 offering and saw no change in student performance as evaluated by a final exam. However, students reported more deep engagement with and understanding of the material, which encourages us to further develop the course.},
booktitle = {Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
pages = {955–961},
numpages = {7},
keywords = {mastery learning, novice programming, cs1, self-paced},
location = {Minneapolis, MN, USA},
series = {SIGCSE '19}
}

@inproceedings{bart2019pythonsneks,
author = {Bart, Austin Cory and Sarver, Allie and Friend, Michael and Cox II, Larry},
title = {PythonSneks: An Open-Source, Instructionally-Designed Introductory Curriculum with Action-Design Research},
year = {2019},
isbn = {9781450358903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287324.3287428},
doi = {10.1145/3287324.3287428},
abstract = {Rising enrollments and limited instructor resources underscores the growing need for reusable, scalable curriculum. In this paper, we describe an open-source introductory Python course for non-Computer Science majors in STEM, designed following best practices of Instructional Design (a process similar to Software Engineering). The created resources include 234 learning objectives, 51 lesson videos, 45 lecture slides, 170 programming problems, 281 quiz questions, 6 unit tested projects, and 4 ethical prompts. A teaching field guide has also been produced as a result of this effort, documenting how to deploy this curriculum on a daily level. We describe our experiences deploying over two semesters. The course serviced over 500 students, with 100s in some sections. Along the way, two interventions were conducted in an Action Design Research style: one using Worked Examples, and another using Structured Small Groups. We report on the mixed results of these experiments, plus evaluations of the assignments from student surveys and statistical measures of item effectiveness. Finally, we describe lessons learned when following Instructional Design processes.},
booktitle = {Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
pages = {307–313},
numpages = {7},
keywords = {open curriculum, python, instructional design},
location = {Minneapolis, MN, USA},
series = {SIGCSE '19}
}

@inproceedings{depontes2019analyzing,
author = {de Pontes, Rafael G. and Guerrero, Dalton D. S. and de Figueiredo, Jorge C. A.},
title = {Analyzing Gamification Impact on a Mastery Learning Introductory Programming Course},
year = {2019},
isbn = {9781450358903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287324.3287367},
doi = {10.1145/3287324.3287367},
abstract = {In recent years, the use of game elements in non-game contexts has been widely adopted in different contexts, one of which education. Concerned with investigating the effect of gamification on student engagement in the context of an undergraduate Introductory Programming course that applied that Mastery Learning approach, a gamified web application was developed and integrated to a pre-existing online platform used to conduct the syllabus. N = 60 students were randomly assigned to one of two groups: an experimental group with access to the gamified platform and a control group without access to it. The randomization guaranteed a similar number of students in relation to advancement in the course, in each group. The main features explored in the gamified approach were anonymous weekly leaderboards, many categories of badges and personal record-tracking. Each student profile was private. A first 90-minute experiment showed that the experimental group solved 37\% more exercises than the control group, on average. From that point onward, student activity was monitored for a period of four weeks, until the end of the course. Though student-activity decreased over time as students completed the course, the experimental group increasingly solved more exercises per week compared to the control group (at least 46\% more, on average). After the course was concluded, the experimental group answered a survey which showed that the badges were the feature that motivated them the most. This research's source code is available and may be easily replicated or incremented.},
booktitle = {Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
pages = {400–406},
numpages = {7},
keywords = {badge, mastery learning, education, programming, gamification, leaderboard},
location = {Minneapolis, MN, USA},
series = {SIGCSE '19}
}

@inproceedings{fernandezreyes2018impact,
author = {Fernandez-Reyes, Kiko and Clarke, Dave and Hornbach, Janina},
title = {The Impact of Opt-in Gamification on Students' Grades in a Software Design Course},
year = {2018},
isbn = {9781450359658},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3270112.3270118},
doi = {10.1145/3270112.3270118},
abstract = {An achievement-driven methodology strives to give students more control of their learning with enough flexibility to engage them in deeper learning.We observed in the course Advanced Software Design, which uses the achievement-driven methodology, that students fail to get high grades, which may hamper deeper learning. To motivate students to pursue and get higher grades we added gamification elements to the course.To measure the success of our gamification implementation, students filled out a questionaire rating the enjoyment and motivation produced by the game. We built a statistical regression model where enjoyment and motivation explain 55\% of the variation in grades. However, only the relationship between motivation and grade is significant, which implies that notivation drives the overall effect of the model. The results suggest that the more the students were motivated by the game, the higher their grades on the course (and vice versa). This implies that if gamification indeed motivates students, then it makes them go beyond what is expected.},
booktitle = {Proceedings of the 21st ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {90–97},
numpages = {8},
keywords = {software design, gamification, education, UML},
location = {Copenhagen, Denmark},
series = {MODELS '18}
}

@inproceedings{gestwicki2018design,
author = {Gestwicki, Paul},
title = {Design and Evaluation of an Undergraduate Course on Software Development Practices},
year = {2018},
isbn = {9781450351034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3159450.3159542},
doi = {10.1145/3159450.3159542},
abstract = {This paper describes the design, evolution, and evaluation of a second-year undergraduate course on professional software development practices. The course design arose from two properties of a curriculum revision: the removal of redundancy across Discrete Mathematics, Data Structures, and Algorithms coursework and the identification of desired skills for students entering the upper-division courses. A research-informed design for the course incorporates code quality, version control, teamwork, user-centered design, risk management, design thinking, and reflective practice, presented in keeping with the values of agile software development. We describe the evolution of the course over its six years of implementation, including a transition from a Java-specific textbook to a more generic book about code quality and the transition from a six-week to a nine-week project. The course has become a linchpin for mid-major assessment of students' software development skills, and we discuss the strengths, weaknesses, and results of our assessment strategy. We provide a reflection of the role this course has had within our program and advice for those who might wish to adopt or adapt this design.},
booktitle = {Proceedings of the 49th ACM Technical Symposium on Computer Science Education},
pages = {221–226},
numpages = {6},
keywords = {clean code, higher education, assessment, course design, education, agile software craftsmanship, test-driven development},
location = {Baltimore, Maryland, USA},
series = {SIGCSE '18}
}

@inproceedings{mccane2017mastery,
author = {McCane, Brendan and Ott, Claudia and Meek, Nick and Robins, Anthony},
title = {Mastery Learning in Introductory Programming},
year = {2017},
isbn = {9781450348232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3013499.3013501},
doi = {10.1145/3013499.3013501},
abstract = {In our first year computer science program we offer an optional first semester Python course to prepare our students for a compulsory second semester Java course. In 2014, we adopted mastery learning as the primary teaching mode for the Python course. We show how the introduction of a mastery learning model in the first semester has had a significant positive impact on student learning especially for weaker students. The structure of our first year allows us to ascribe the differences to the mastery learning model, rather than other confounding factors. We also report on the students' reaction to the mastery learning model.},
booktitle = {Proceedings of the Nineteenth Australasian Computing Education Conference},
pages = {1–10},
numpages = {10},
keywords = {Introductory Programming, CS1, Mastery Learning, Python},
location = {Geelong, VIC, Australia},
series = {ACE '17}
}

@inproceedings{urbanlurain1999ido,
author = {Urban-Lurain, Mark and Weinshank, Donald J.},
title = {“I Do and I Understand”: Mastery Model Learning for a Large Non-Major Course},
year = {1999},
isbn = {1581130856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/299649.299738},
doi = {10.1145/299649.299738},
abstract = {We describe the infrastructure we have created for a large enrollment (1700 / semester) non-major course. The course combines collaborative, lab-based problem-solving instruction with modified mastery-learning assessment. The infrastructure supports continuous improvement to respond to client department needs, incoming student experience, course design flaws and computing platform upgrades.},
booktitle = {The Proceedings of the Thirtieth SIGCSE Technical Symposium on Computer Science Education},
pages = {150–154},
numpages = {5},
keywords = {mastery learning, assessments, Web-based techniques, non-majors, large enrollment},
location = {New Orleans, Louisiana, USA},
series = {SIGCSE '99}
}



@article{isomottonen2016flipping,
author = {Isom\"{o}tt\"{o}nen, Ville and Tirronen, Ville},
title = {Flipping and Blending—An Action Research Project on Improving a Functional Programming Course},
year = {2016},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
url = {https://doi.org/10.1145/2934697},
doi = {10.1145/2934697},
abstract = {This article reports on an action research project on improving a functional programming course by moving toward a practical and flexible study environment—flipped and blended classroom. Teaching the topic of functional programming was found to be troublesome using a traditional lectured course format. The need to increase students’ amount of practice emerged while subsequent challenges relating to students’ independent practical coursework were observed. Particular concerns relating to group work, learning materials, and the attribute of flexibility were investigated during the third action research cycle. The research cycle was analyzed using a qualitative survey on students’ views, teacher narrative, and students’ study activity data. By this third research cycle, we found that (i) the “call for explanation” is an apt conceptualization for supporting independent work, and in particular for the design of learning materials; (ii) use of student-selected groups that can be flexibly resized or even disbanded enables spontaneous peer support and can avoid frustration about group work; and (iii) students greatly appreciate the high degree of flexibility in the course arrangements but find that it causes them to slip from their goals. The project has improved our understanding of a successful implementation of the target course based on group work and learning materials in the context of independent study, while the attribute of flexibility revealed a contradiction that indicates the need for further action.},
journal = {ACM Trans. Comput. Educ.},
month = {sep},
articleno = {1},
numpages = {35},
keywords = {blended learning, functional programming, Action research, flipped classroom, independent study}
}

@inproceedings{young2012evaluation,
author = {Young, P. and Yip, V. and Lenin, R. B.},
title = {Evaluation of Issue-Tracker's Effectiveness for Measuring Individual Performance on Group Projects},
year = {2012},
isbn = {9781450312035},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2184512.2184534},
doi = {10.1145/2184512.2184534},
abstract = {Group-based software engineering projects are an important part of Computing curricula. Evaluating the overall performance of the group on such a project is fairly straightforward; however, fair assessment of individual contributions to group-based projects remains a challenging problem. In an attempt to overcome this challenge, the web-based task management system, Issue-Tracker, has been adopted for evaluating a semester-long project involving an entire software engineering class. While Issue-Tracker appeared to be a useful tool for managing the overall project, the question of its effectiveness in evaluating the individual performance of a student remained. As an initial evaluation of Issue-Tracker's effectiveness, a subjective comparison of Issue-Tracker and an alternative grading method, competency matrices for peer assessment, was provided in previous work by the authors. This paper provides an objective comparison of the Issue-Tracker and competency matrix approaches by computing Spearman's and Pearson's correlation coefficients to see if scores obtained using the two methods are significantly correlated.},
booktitle = {Proceedings of the 50th Annual Southeast Regional Conference},
pages = {89–94},
numpages = {6},
keywords = {tools, research study, assessment, group projects, competency matrix, correlation, software engineering, issue-tracker, experience report, classroom management, peer assessment},
location = {Tuscaloosa, Alabama},
series = {ACM-SE '12}
}

@inproceedings{eagle2012learning,
author = {Eagle, Michael John and Barnes, Tiffany},
title = {A Learning Objective Focused Methodology for the Design and Evaluation of Game-Based Tutors},
year = {2012},
isbn = {9781450310987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2157136.2157170},
doi = {10.1145/2157136.2157170},
abstract = {We present the Game2Learn methodology for the design and evaluation of educational games with a focus on well-defined learning objectives and empirical verification. This integrative process adapts ideas from educational design, intelligent tutoring systems, classical test-theory, and interaction and game design, and agile software development. The methodology guides researchers through the steps of the design process, including identification of specific learning objectives, translation of learning activities to game mechanics, and the empirical evaluation of the final product. This methodology is particularly useful for ensuring successful student research experiences or software engineering courses.},
booktitle = {Proceedings of the 43rd ACM Technical Symposium on Computer Science Education},
pages = {99–104},
numpages = {6},
keywords = {empirical evaluation, educational games, design methods, serious games, game design},
location = {Raleigh, North Carolina, USA},
series = {SIGCSE '12}
}

@article{shaffer2005ludwig,
author = {Shaffer, Steven C.},
title = {Ludwig: An Online Programming Tutoring and Assessment System},
year = {2005},
issue_date = {June 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {0097-8418},
url = {https://doi.org/10.1145/1083431.1083464},
doi = {10.1145/1083431.1083464},
abstract = {An online programming tutoring and assessment system is described, and the results of a preliminary study are presented. Ten students in an introductory C++ programming course used the system; both qualitative and quantitative data were collected and suggest that a future large-scale implementation will yield beneficial results},
journal = {SIGCSE Bull.},
month = {jun},
pages = {56–60},
numpages = {5},
keywords = {web-based, programming, tutoring, assessment, online}
}


@article{lam2021successful,
author = {Lam, Michael O. and Weikle, Dee A. B.},
title = {A Successful Online Systems Class Using Scaffolded Active Learning and Formative Assessment},
year = {2021},
issue_date = {October 2021},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {37},
number = {3},
issn = {1937-4771},
abstract = {This last year has been a challenge for faculty transitioning from in-person instruction to online instruction. We approached the semester with serious concerns but discovered our hybrid course transitioned well to being completely online with a few key modifications: video versions of all lectures, Google slide implementations of in-class labs, course procedures that allowed students to choose their breakout room in Zoom, and randomization of question selection for midterm and final exams. In person, this course made thoughtful use of active learning and formative assessment to scaffold students into summative assessment. The online version does the same and, in our opinion, this contributed to the online success. This paper discusses the scaffolding formative assessments, how they build to the summative module tests and midterm and final exams as well as how we implemented active learning online. We also provide links to the formative labs and videos that made the active learning work.},
journal = {J. Comput. Sci. Coll.},
month = {oct},
pages = {132–142},
numpages = {11}
}